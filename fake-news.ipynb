{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1  \n",
    "\n",
    "Pandas is used to process The fake news corpus. Since content will be used for our models we drop any rows that don't have any content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1344\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1345\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1346\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1327\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1373\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1373\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1322\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1322\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1081\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1081\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1083\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1084\u001b[0m \n\u001b[1;32m   1085\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1025\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m-> 1025\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1026\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1468\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     server_hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m-> 1468\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mwrap_socket(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock,\n\u001b[1;32m   1469\u001b[0m                                       server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    456\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    457\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    458\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    459\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    460\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    461\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    462\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    463\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1046\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1047\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1321\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1321\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1322\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/clarabovingmagnussen/Desktop/GDS/Projekt/fake-news.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/clarabovingmagnussen/Desktop/GDS/Projekt/fake-news.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpandas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/clarabovingmagnussen/Desktop/GDS/Projekt/fake-news.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/clarabovingmagnussen/Desktop/GDS/Projekt/fake-news.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(url)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/clarabovingmagnussen/Desktop/GDS/Projekt/fake-news.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mhead)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    729\u001b[0m     path_or_buf,\n\u001b[1;32m    730\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    731\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    732\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    733\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    734\u001b[0m )\n\u001b[1;32m    736\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    383\u001b[0m req_info \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[39m=\u001b[39mstorage_options)\n\u001b[0;32m--> 384\u001b[0m \u001b[39mwith\u001b[39;00m urlopen(req_info) \u001b[39mas\u001b[39;00m req:\n\u001b[1;32m    385\u001b[0m     content_encoding \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Encoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    386\u001b[0m     \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m         \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39mthe stdlib.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlopen(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    514\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 515\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    517\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    518\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    531\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 532\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    533\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    534\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    535\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    491\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    493\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1392\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[1;32m   1393\u001b[0m                         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1347\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1344\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[1;32m   1345\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   1346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1348\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m   1349\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "df = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/GDS/Projekt/news_sample.csv\")\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've implemented data processing functions to do the following:\n",
    "\n",
    "    - Clean the text\n",
    "    - Tokenize the text\n",
    "    - Remove stopwords\n",
    "    - Remove word variations with stemming\n",
    "We use nltk and cleantext because it has built-in support for many of these operations.\n",
    "We also use collections to import a counter, sklearn to import functions to split the dataset, chain to help with counting and matplotlib for visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from cleantext import clean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def plot_word_frequency(counter, top_n=10000, title=\"Word Frequency Distribution\"):\n",
    "    \"\"\"\n",
    "    Plots the frequency distribution of the top_n words using a log-log plot.\n",
    "    \"\"\"\n",
    "    # Extract frequencies of the most common words\n",
    "    freqs = [freq for word, freq in counter.most_common(top_n)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(range(1, len(freqs) + 1), freqs, marker=\".\")\n",
    "    plt.xlabel(\"Rank of word (log scale)\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Regex pattern for tokenization\n",
    "# <\\w+> matches tags (e.g., <num>), [\\w]+(?:-[\\w]+)? matches words with hyphens\n",
    "pattern = r'<\\w+>|[\\w]+(?:-[\\w]+)?'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    clean_text = re.sub(\n",
    "        r'([A-Za-z]+\\.?\\s[0-9]{1,2}?,\\s[0-9]{4})|\\b\\d{4}-\\d{2}-\\d{2}\\b|\\b\\d{2}-\\d{2}-\\d{4}\\b', \n",
    "        '<DATE>', \n",
    "        text\n",
    "    )\n",
    "    clean_text = clean(clean_text,\n",
    "        lower=True,\n",
    "        no_urls=True, replace_with_url=\"<URL>\",\n",
    "        no_emails=True, replace_with_email=\"<EMAIL>\",\n",
    "        no_numbers=True, replace_with_number= r\"<NUM>\",\n",
    "        no_currency_symbols=True, replace_with_currency_symbol=\"<CUR>\",\n",
    "        no_punct=True, replace_with_punct=\"\",\n",
    "        no_line_breaks=True \n",
    "    )\n",
    "    return clean_text\n",
    "\n",
    "def tokenize_text(text: str, stop_words: set) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using NLTK and removes stopwords.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def stem_tokens(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Applies Porter stemming to a list of tokens.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def split_dataset(df, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train_df, temp_df = train_test_split(df, test_size=(1 - train_ratio), random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=val_ratio/(1 - train_ratio), random_state=42)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def main(df, text_columns):\n",
    "    \"\"\"\n",
    "    Processes text data by cleaning, tokenizing, and stemming.\n",
    "    This version handles multiple text columns at once by combining them.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        text_columns (str or list): Name of the text column or a list of text column names.\n",
    "    \"\"\"\n",
    "    # Ensure text_columns is a list\n",
    "    if isinstance(text_columns, str):\n",
    "        text_columns = [text_columns]\n",
    "    \n",
    "    # Create a combined text column by joining the specified columns (ignoring any NaNs)\n",
    "    df['combined_text'] = df[text_columns].apply(lambda row: \" \".join(row.dropna().astype(str)), axis=1)\n",
    "    \n",
    "    # Define English stopwords set\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Clean the combined text data\n",
    "    df['cleaned_text'] = df['combined_text'].apply(clean_text)\n",
    "    \n",
    "    # Tokenize the cleaned text and remove stopwords\n",
    "    df['tokenized_text'] = df['cleaned_text'].apply(lambda x: \" \".join(tokenize_text(x, stop_words)))\n",
    "    \n",
    "    # Save tokenized text to CSV (the file name reflects that columns were combined)\n",
    "    df['tokenized_text'].to_csv(\"combined_tokenized_news_sample.csv\", index=False)\n",
    "    \n",
    "    # Count word frequencies in the tokenized text\n",
    "    tokenized_series = df['tokenized_text']\n",
    "    all_tokens = list(chain.from_iterable(tokenized_series.str.split()))\n",
    "    token_freq = Counter(all_tokens)\n",
    "    \n",
    "    # Count word frequencies in the cleaned (lowercased) text for comparison\n",
    "    original_tokens = list(chain.from_iterable(df['cleaned_text'].str.split()))\n",
    "    original_freq = Counter(original_tokens)\n",
    "    \n",
    "    # Compute vocabulary sizes and reduction rate after stopword removal\n",
    "    vocab_original = set(original_tokens)\n",
    "    vocab_tokenized = set(all_tokens)\n",
    "    reduction_rate_stopwords = (len(vocab_original) - len(vocab_tokenized)) / len(vocab_original)\n",
    "    \n",
    "    print(\"----- Vocabulary Analysis -----\")\n",
    "    print(f\"Original vocabulary size: {len(vocab_original)}\")\n",
    "    print(f\"Vocabulary size after stopword removal: {len(vocab_tokenized)}\")\n",
    "    print(f\"Reduction rate after stopword removal: {reduction_rate_stopwords:.2%}\")\n",
    "    \n",
    "    # Apply stemming to the tokenized text\n",
    "    df['stemmed_text'] = df['tokenized_text'].apply(lambda x: \" \".join(stem_tokens(x.split())))\n",
    "    df['stemmed_text'].to_csv(\"combined_stemmed_news_sample.csv\", index=False)\n",
    "    \n",
    "    # Count word frequencies in the stemmed text\n",
    "    all_stemmed_tokens = list(chain.from_iterable(df['stemmed_text'].str.split()))\n",
    "    stem_freq = Counter(all_stemmed_tokens)\n",
    "    \n",
    "    # Compute vocabulary size and reduction rate after stemming\n",
    "    vocab_stemmed = set(all_stemmed_tokens)\n",
    "    reduction_rate_stemming = (len(vocab_tokenized) - len(vocab_stemmed)) / len(vocab_tokenized)\n",
    "    \n",
    "    print(\"\\n----- Stemming Analysis -----\")\n",
    "    print(f\"Vocabulary size after stemming: {len(vocab_stemmed)}\")\n",
    "    print(f\"Reduction rate after stemming: {reduction_rate_stemming:.2%}\")\n",
    "    \n",
    "    # Print top 10 words for each version of the text\n",
    "    print(\"\\n----- Top 10 Words -----\")\n",
    "    print(f\"Original sample: {original_freq.most_common(10)}\")\n",
    "    print(f\"Tokenized (stopword-removed) sample: {token_freq.most_common(10)}\")\n",
    "    print(f\"Stemmed sample: {stem_freq.most_common(10)}\")\n",
    "    \n",
    "    # Count placeholder tokens (note: the clean function lowercases text)\n",
    "    url_count = df['stemmed_text'].str.count(\"<url>\").sum()\n",
    "    date_count = df['stemmed_text'].str.count(\"<date>\").sum()\n",
    "    num_count = df['stemmed_text'].str.count(\"<num>\").sum()\n",
    "    \n",
    "    print(\"\\n----- Placeholder Token Counts -----\")\n",
    "    print(f\"Number of URLs: {url_count}\")\n",
    "    print(f\"Number of dates: {date_count}\")\n",
    "    print(f\"Number of numerics: {num_count}\")\n",
    "    \n",
    "    # Plot frequency distributions\n",
    "    plot_word_frequency(original_freq, top_n=10000, title=\"Original Text Frequency Distribution\")\n",
    "    plot_word_frequency(token_freq, top_n=10000, title=\"Stopword-Removed Frequency Distribution\")\n",
    "    plot_word_frequency(stem_freq, top_n=10000, title=\"Stemmed Frequency Distribution\")\n",
    "    \n",
    "    print(\"\\n----- DataFrame Summary -----\")\n",
    "    print(df.info())\n",
    "    print(df.describe(include=\"all\"))\n",
    "    \n",
    "    # Split the DataFrame into training, validation, and test sets\n",
    "    train_df, val_df, test_df = split_dataset(df)\n",
    "    print(\"\\nDataset split sizes:\")\n",
    "    print(f\"Training set: {len(train_df)} rows\")\n",
    "    print(f\"Validation set: {len(val_df)} rows\")\n",
    "    print(f\"Test set: {len(test_df)} rows\")\n",
    "\n",
    "    train_df, val_df, test_df = split_dataset(df)\n",
    "    return train_df, val_df, test_df  # Returner datasættene\n",
    "\n",
    "# Read the CSV file into a DataFrame and select a subset (for testing)\n",
    "df = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/GDS/Projekt/news_sample.csv\", encoding=\"utf-8\")\n",
    "df_sample = df.head(300).copy()  # For a large dataset, consider processing in chunks\n",
    "main(df_sample, text_columns='content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "We apply our data processing pipeline from task 1 on the 995k FakeNewsCorpus.\n",
    "\n",
    "Since Pandas is slow on larger datasets we can use modin and ray to optimize pandas and allow for multithreading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.config as modin_config\n",
    "import modin.pandas as pd\n",
    "modin_config.Engine.put('ray')\n",
    "\n",
    "# only read the columns we need\n",
    "df = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/GDS/Projekt/995000_rows.csv\", \n",
    "                 usecols=['content', 'type', 'title', 'domain'], \n",
    "                 engine='c', \n",
    "                 dtype = str)\n",
    "df = df.dropna(subset=['content', 'type', 'title'])\n",
    "main(df.head(300).copy(), text_columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2  \n",
    "### Task 1  \n",
    "\n",
    "To build a baseline model for Fake News classification, we implemented a **logistic regression model** using a vocabulary of the 10,000 most frequently occurring words.  \n",
    "\n",
    "We used `CountVectorizer` to transform the text data into numerical features. The model was trained on the dataset to distinguish between **reliable** and **fake** news articles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "def categorize_type(df):\n",
    "    \"\"\" Categorize types in omitted, fake, reliable \"\"\"\n",
    "    omitted = {'hate', 'bias', 'satire', 'unreliable', 'state'}\n",
    "    fake = {'fake', 'junksci', 'conspiracy'}\n",
    "    reliable = {'reliable', 'political', 'clickbait'}\n",
    "\n",
    "    df_categorized = df[~df['type'].isin(omitted)].copy()\n",
    "    df_categorized['type'] = df_categorized['type'].map(lambda x: 'fake' if x in fake else ('reliable' if x in reliable else None))\n",
    "    df_categorized = df_categorized.dropna(subset=['type'])\n",
    "\n",
    "    return df_categorized\n",
    "\n",
    "# Rens FakeNewsCorpus datasættet \n",
    "df = categorize_type(df)\n",
    "\n",
    "# Load articles_clara.csv \n",
    "scraped_df = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/GDS/Projekt/articles_clara.csv\", encoding=\"utf-8\")\n",
    "\n",
    "print(scraped_df.columns)\n",
    "# Sikre at nødvendige kolonner findes i `clara_df`\n",
    "if 'text' in scraped_df.columns:\n",
    "    scraped_df = scraped_df.rename(columns={'text': 'content'})\n",
    "    scraped_df['type'] = 'reliable'  # Marker artikler i Articels_clara som pålidelige\n",
    "    scraped_df = scraped_df[['content', 'type']]\n",
    "else:\n",
    "    raise ValueError(\"Fejl: Kolonnen 'text' mangler i articles_clara.csv\")\n",
    "\n",
    "print(\"Type of df:\", type(df))  # Ensure df is a DataFrame\n",
    "print(\"Type of scraped_df:\", type(scraped_df))  # Ensure scraped_df is a DataFrame\n",
    "\n",
    "# Convert Modin DataFrame to Pandas before concatenation\n",
    "df = df._to_pandas() \n",
    "\n",
    "# Sammenlæg de to datasæt\n",
    "combined_train_df = pd.concat([df, scraped_df], ignore_index=True)\n",
    "\n",
    "# Opdel data i træning (80%), validering (10%) og test (10%)\n",
    "train_df, val_df, test_df = np.split(combined_train_df.sample(frac=1, random_state=42), \n",
    "                                     [int(0.8*len(combined_train_df)), int(0.9*len(combined_train_df))])\n",
    "\n",
    "# Brug CountVectorizer til at transformere tekst til numeriske features\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(train_df['content'])\n",
    "X_test = vectorizer.transform(test_df['content'])\n",
    "\n",
    "# Konverter labels til binære værdier (1 = Fake, 0 = Reliable)\n",
    "y_train = np.array([1 if label == \"fake\" else 0 for label in train_df['type']])\n",
    "y_test = np.array([1 if label == \"fake\" else 0 for label in test_df['type']])\n",
    "\n",
    "# Træn en simpel logistisk regressionsmodel\n",
    "clf = LogisticRegression(max_iter=500, solver='saga')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Lav forudsigelser på testdata\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluer modellen\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model performance after integrating Articles_Clara dataset:\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"F1 Score: {f1:.2%}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract TP, TN, FP, FN\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Udskriv resultater\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Meta data tilføjes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Use TF-IDF Vectorizer to transform text into a vocabulary of the 10,000 most frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "\n",
    "# Convert text content into numerical feature matrices\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['content'])\n",
    "X_test_tfidf = vectorizer.transform(test_df['content'])\n",
    "\n",
    "# Convert labels to binary values (1 = Fake, 0 = Reliable)\n",
    "y_train_tfidf = np.array([1 if label == \"fake\" else 0 for label in train_df['type']])\n",
    "y_test_tfidf = np.array([1 if label == \"fake\" else 0 for label in test_df['type']])\n",
    "\n",
    "model = LinearSVC( C=1.0, random_state = 0)\n",
    "model.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "y_pred_tfidf=model.predict(X_test_tfidf)\n",
    "y_pred_train_tfidf=model.predict(X_train_tfidf)\n",
    "\n",
    "# Evaluer modellen\n",
    "accuracy_tfidf =accuracy_score(y_pred_tfidf, y_test_tfidf)\n",
    "f1_tfidf =accuracy_score(y_pred_train_tfidf, y_train_tfidf )\n",
    "\n",
    "\n",
    "print(\"Model performance after integrating Articles_Clara dataset:\")\n",
    "print(f\"Accuracy: {accuracy_tfidf:.2%}\")\n",
    "print(f\"F1 Score: {f1_tfidf:.2%}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm_tfidf = confusion_matrix(y_test_tfidf, y_pred_tfidf)\n",
    "\n",
    "# Extract TP, TN, FP, FN\n",
    "tn, fp, fn, tp = cm_tfidf.ravel()\n",
    "\n",
    "# Udskriv resultater\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_tfidf)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_tfidf, y_pred_tfidf))\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "#tanke: SVM arbejdere i høje dimensioner men for at vi kan visualsiere det bliver man nødt til at reducere datsættet så man kan vise det i 2D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA #Libary to convert from higher to lower dimensions\n",
    "\n",
    "#Convert to close matrix\n",
    "X_train_dense = X_train_tfidf.toarray()\n",
    "\n",
    "#Convert to 2D\n",
    "pca= PCA(n_components= 2) #sikre 2D\n",
    "X_pca=pca.fit_transform(X_train_dense)\n",
    "\n",
    "#Train linear SVM on close 2D mateix\n",
    "model_pca = LinearSVC(C=1.0, random_state=0)\n",
    "model_pca.fit(X_train_dense,y_train_tfidf)\n",
    "\n",
    "#Get weights and bias\n",
    "w = model_pca.coef_[0] \n",
    "b = model_pca.intercept_[0] #bias\n",
    "\n",
    "#Compute decision boundary\n",
    "xx = np.linspace(model_pca[:, 0].min(), model_pca[:, 0].max(), 100)\n",
    "yy = -(w[0] / w[1]) * xx - (b / w[1])\n",
    "\n",
    "# Figure out the PCA parameters\n",
    "print(\"Explained Variance Ratio of each component:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(\"\\nPCA Components (Loading vectors):\")\n",
    "print(pca.components_)\n",
    "\n",
    "#Plot\n",
    "#plt.scatter(model_pca[:, 0],model_pca[:, 1], c=y_train_tfidf, cmap='coolwarm', edgecolors='k')\n",
    "#plt.plot(xx,yy, 'k--')\n",
    "#plt.xlabel('x-akse')\n",
    "#plt.ylabel('y-akse')\n",
    "#plt.titel('Visualisation of SVM linear model(2D)')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
