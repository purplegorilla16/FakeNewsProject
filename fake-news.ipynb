{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### Task 1  \n",
    "\n",
    "Pandas is used to process The fake news corpus. Since content will be used for our models we drop any rows that don't have any content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "df = pd.read_csv(url)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation of task 1\n",
    "\n",
    "We've implemented data processing functions to do the following:\n",
    "\n",
    "    - Clean the text\n",
    "    - Tokenize the text\n",
    "    - Remove stopwords\n",
    "    - Remove word variations with stemming\n",
    "We use nltk and cleantext because it has built-in support for many of these operations.\n",
    "We also use collections to import a counter, sklearn to import functions to split the dataset, chain to help with counting and matplotlib for visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from cleantext import clean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def plot_word_frequency(counter, top_n=10000, title=\"Word Frequency Distribution\"):\n",
    "    \"\"\"\n",
    "    Plots the frequency distribution of the top_n words using a log-log plot.\n",
    "    \"\"\"\n",
    "    # Extract frequencies of the most common words\n",
    "    freqs = [freq for word, freq in counter.most_common(top_n)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(range(1, len(freqs) + 1), freqs, marker=\".\")\n",
    "    plt.xlabel(\"Rank of word (log scale)\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Regex pattern for tokenization\n",
    "# <\\w+> matches tags (e.g., <num>), [\\w]+(?:-[\\w]+)? matches words with hyphens\n",
    "pattern = r'<\\w+>|[\\w]+(?:-[\\w]+)?'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    clean_text = re.sub(\n",
    "        r'([A-Za-z]+\\.?\\s[0-9]{1,2}?,\\s[0-9]{4})|\\b\\d{4}-\\d{2}-\\d{2}\\b|\\b\\d{2}-\\d{2}-\\d{4}\\b', \n",
    "        '<DATE>', \n",
    "        text\n",
    "    )\n",
    "    clean_text = clean(clean_text,\n",
    "        lower=True,\n",
    "        no_urls=True, replace_with_url=\"<URL>\",\n",
    "        no_emails=True, replace_with_email=\"<EMAIL>\",\n",
    "        no_numbers=True, replace_with_number= r\"<NUM>\",\n",
    "        no_currency_symbols=True, replace_with_currency_symbol=\"<CUR>\",\n",
    "        no_punct=True, replace_with_punct=\"\",\n",
    "        no_line_breaks=True \n",
    "    )\n",
    "    return clean_text\n",
    "\n",
    "def tokenize_text(text: str, stop_words: set) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using NLTK and removes stopwords.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def stem_tokens(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Applies Porter stemming to a list of tokens.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def split_dataset(df, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train_df, temp_df = train_test_split(df, test_size=(1 - train_ratio), random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=val_ratio/(1 - train_ratio), random_state=42)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def main(df, text_columns):\n",
    "    \"\"\"\n",
    "    Processes text data by cleaning, tokenizing, and stemming.\n",
    "    This version handles multiple text columns at once by combining them.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        text_columns (str or list): Name of the text column or a list of text column names.\n",
    "    \"\"\"\n",
    "    # Ensure text_columns is a list\n",
    "    if isinstance(text_columns, str):\n",
    "        text_columns = [text_columns]\n",
    "    \n",
    "    # Create a combined text column by joining the specified columns (ignoring any NaNs)\n",
    "    df['combined_text'] = df[text_columns].apply(lambda row: \" \".join(row.dropna().astype(str)), axis=1)\n",
    "    \n",
    "    # Define English stopwords set\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Clean the combined text data\n",
    "    df['cleaned_text'] = df['combined_text'].apply(clean_text)\n",
    "    \n",
    "    # Tokenize the cleaned text and remove stopwords\n",
    "    df['tokenized_text'] = df['cleaned_text'].apply(lambda x: \" \".join(tokenize_text(x, stop_words)))\n",
    "    \n",
    "    # Save tokenized text to CSV (the file name reflects that columns were combined)\n",
    "    df['tokenized_text'].to_csv(\"combined_tokenized_news_sample.csv\", index=False)\n",
    "    \n",
    "    # Count word frequencies in the tokenized text\n",
    "    tokenized_series = df['tokenized_text']\n",
    "    all_tokens = list(chain.from_iterable(tokenized_series.str.split()))\n",
    "    token_freq = Counter(all_tokens)\n",
    "    \n",
    "    # Count word frequencies in the cleaned (lowercased) text for comparison\n",
    "    original_tokens = list(chain.from_iterable(df['cleaned_text'].str.split()))\n",
    "    original_freq = Counter(original_tokens)\n",
    "    \n",
    "    # Compute vocabulary sizes and reduction rate after stopword removal\n",
    "    vocab_original = set(original_tokens)\n",
    "    vocab_tokenized = set(all_tokens)\n",
    "    reduction_rate_stopwords = (len(vocab_original) - len(vocab_tokenized)) / len(vocab_original)\n",
    "    \n",
    "    print(\"----- Vocabulary Analysis -----\")\n",
    "    print(f\"Original vocabulary size: {len(vocab_original)}\")\n",
    "    print(f\"Vocabulary size after stopword removal: {len(vocab_tokenized)}\")\n",
    "    print(f\"Reduction rate after stopword removal: {reduction_rate_stopwords:.2%}\")\n",
    "    \n",
    "    # Apply stemming to the tokenized text\n",
    "    df['stemmed_text'] = df['tokenized_text'].apply(lambda x: \" \".join(stem_tokens(x.split())))\n",
    "    df['stemmed_text'].to_csv(\"combined_stemmed_news_sample.csv\", index=False)\n",
    "    \n",
    "    # Count word frequencies in the stemmed text\n",
    "    all_stemmed_tokens = list(chain.from_iterable(df['stemmed_text'].str.split()))\n",
    "    stem_freq = Counter(all_stemmed_tokens)\n",
    "    \n",
    "    # Compute vocabulary size and reduction rate after stemming\n",
    "    vocab_stemmed = set(all_stemmed_tokens)\n",
    "    reduction_rate_stemming = (len(vocab_tokenized) - len(vocab_stemmed)) / len(vocab_tokenized)\n",
    "    \n",
    "    print(\"\\n----- Stemming Analysis -----\")\n",
    "    print(f\"Vocabulary size after stemming: {len(vocab_stemmed)}\")\n",
    "    print(f\"Reduction rate after stemming: {reduction_rate_stemming:.2%}\")\n",
    "    \n",
    "    # Print top 10 words for each version of the text\n",
    "    print(\"\\n----- Top 10 Words -----\")\n",
    "    print(f\"Original sample: {original_freq.most_common(10)}\")\n",
    "    print(f\"Tokenized (stopword-removed) sample: {token_freq.most_common(10)}\")\n",
    "    print(f\"Stemmed sample: {stem_freq.most_common(10)}\")\n",
    "    \n",
    "    # Count placeholder tokens (note: the clean function lowercases text)\n",
    "    url_count = df['stemmed_text'].str.count(\"<url>\").sum()\n",
    "    date_count = df['stemmed_text'].str.count(\"<date>\").sum()\n",
    "    num_count = df['stemmed_text'].str.count(\"<num>\").sum()\n",
    "    \n",
    "    print(\"\\n----- Placeholder Token Counts -----\")\n",
    "    print(f\"Number of URLs: {url_count}\")\n",
    "    print(f\"Number of dates: {date_count}\")\n",
    "    print(f\"Number of numerics: {num_count}\")\n",
    "    \n",
    "    # Plot frequency distributions\n",
    "    plot_word_frequency(original_freq, top_n=10000, title=\"Original Text Frequency Distribution\")\n",
    "    plot_word_frequency(token_freq, top_n=10000, title=\"Stopword-Removed Frequency Distribution\")\n",
    "    plot_word_frequency(stem_freq, top_n=10000, title=\"Stemmed Frequency Distribution\")\n",
    "    \n",
    "    print(\"\\n----- DataFrame Summary -----\")\n",
    "    print(df.info())\n",
    "    print(df.describe(include=\"all\"))\n",
    "    \n",
    "    # Split the DataFrame into training, validation, and test sets\n",
    "    train_df, val_df, test_df = split_dataset(df)\n",
    "    print(\"\\nDataset split sizes:\")\n",
    "    print(f\"Training set: {len(train_df)} rows\")\n",
    "    print(f\"Validation set: {len(val_df)} rows\")\n",
    "    print(f\"Test set: {len(test_df)} rows\")\n",
    "\n",
    "    train_df, val_df, test_df = split_dataset(df)\n",
    "    return train_df, val_df, test_df  # Returner datas√¶ttene\n",
    "\n",
    "# Read the CSV file into a DataFrame and select a subset (for testing)\n",
    "df = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/GDS/Projekt/news_sample.csv\", encoding=\"utf-8\")\n",
    "df_sample = df.head(300).copy()  # For a large dataset, consider processing in chunks\n",
    "main(df_sample, text_columns='content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### Task 2\n",
    "\n",
    "We apply our data processing pipeline from task 1 on the 995k FakeNewsCorpus.\n",
    "\n",
    "Since Pandas is slow on larger datasets we can use modin and ray to optimize pandas and allow for multithreading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.config as modin_config\n",
    "import modin.pandas as pd\n",
    "modin_config.Engine.put('ray')\n",
    "\n",
    "# only read the columns we need\n",
    "df = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/GDS/Projekt/995000_rows.csv\", \n",
    "                 usecols=['content', 'type', 'title', 'domain'], \n",
    "                 engine='c', \n",
    "                 dtype = str)\n",
    "df = df.dropna(subset=['content', 'type', 'title'])\n",
    "main(df.head(300).copy(), text_columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2  \n",
    "### Task 1, Task 2 and Task 3\n",
    "\n",
    "To create a baseline model for Fake News classification, we implemented a **logistic regression model** using a vocabulary of the 10,000 most frequently occurring words.  \n",
    "\n",
    "We used `CountVectorizer` to transform the text data into numerical features. The model was trained on the dataset to distinguish between **reliable** and **fake** news articles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def categorize_type(df):\n",
    "    \"\"\" Categorize types in omitted, fake, reliable \"\"\"\n",
    "    omitted = {'hate', 'bias', 'satire', 'unreliable', 'state'}\n",
    "    fake = {'fake', 'junksci', 'conspiracy'}\n",
    "    reliable = {'reliable', 'political', 'clickbait'}\n",
    "\n",
    "    df_categorized = df[~df['type'].isin(omitted)].copy()\n",
    "    df_categorized['type'] = df_categorized['type'].map(lambda x: 'fake' if x in fake else ('reliable' if x in reliable else None))\n",
    "    df_categorized = df_categorized.dropna(subset=['type'])\n",
    "\n",
    "    return df_categorized\n",
    "\n",
    "# Rens FakeNewsCorpus datas√¶ttet \n",
    "df = categorize_type(df)\n",
    "\n",
    "# Load articles_clara.csv \n",
    "scraped_df = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/GDS/Projekt/articles_clara.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Checking that neccesarry comlumns exsists in 'df'\n",
    "if 'text' in scraped_df.columns:\n",
    "    scraped_df = scraped_df.rename(columns={'text': 'content'})\n",
    "    scraped_df['type'] = 'reliable'  # Marker artikler i Articels_clara som p√•lidelige\n",
    "    scraped_df = scraped_df[['content', 'type']]\n",
    "else:\n",
    "    raise ValueError(\"Fejl: Kolonnen 'text' mangler i articles_clara.csv\")\n",
    "\n",
    "print(\"Type of df:\", type(df))  # Ensure df is a DataFrame\n",
    "print(\"Type of scraped_df:\", type(scraped_df))  # Ensure scraped_df is a DataFrame\n",
    "\n",
    "# Convert Modin DataFrame to Pandas before concatenation\n",
    "df = df._to_pandas() \n",
    "\n",
    "# Concat the two datasets\n",
    "combined_train_df = pd.concat([df, scraped_df], ignore_index=True)\n",
    "\n",
    "# Split data i training (80%), validationg (10%) and test (10%) sets\n",
    "train_df, val_df, test_df = np.split(combined_train_df.sample(frac=1, random_state=42), \n",
    "                                     [int(0.8*len(combined_train_df)), int(0.9*len(combined_train_df))])\n",
    "\n",
    "# Use CountVectorizer to transform text into numeric features\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(train_df['content'])\n",
    "X_test = vectorizer.transform(test_df['content'])\n",
    "\n",
    "# Convert labels to binary values (1 = Fake, 0 = Reliable)\n",
    "y_train = np.array([1 if label == \"fake\" else 0 for label in train_df['type']])\n",
    "y_test = np.array([1 if label == \"fake\" else 0 for label in test_df['type']])\n",
    "\n",
    "# Train simpel logistisk regressionsmodel\n",
    "clf = LogisticRegression(max_iter=500, solver='saga')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on testdata\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "dump(vectorizer, \"count_vectorizer.pkl\")\n",
    "dump(clf, \"logistic_model.pkl\")\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model performance after integrating Articles_Clara dataset:\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"F1 Score: {f1:.2%}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract TP, TN, FP, FN\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "For our advanced Fake News predictor, we implemented a **Support Vector Machine (SVM) with a linear kernel** using **TF-IDF** as a feature. \n",
    "\n",
    "We use `TfidfVectorizer` to convert text into numerical feature representations, reducing the impact of frequently occurring words while emphasizing informative words that differentiate fake from reliable news.\n",
    "\n",
    "The model is trained using **a vocabulary of the 10,000 most frequent words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "\n",
    "# Use TF-IDF Vectorizer to transform text into a vocabulary of the 10,000 most frequent words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
    "\n",
    "# Convert text content into numerical feature matrices\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['content'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['content'])\n",
    "\n",
    "# Convert labels to binary values (1 = Fake, 0 = Reliable)\n",
    "y_train_tfidf = np.array([1 if label == \"fake\" else 0 for label in train_df['type']])\n",
    "y_test_tfidf = np.array([1 if label == \"fake\" else 0 for label in test_df['type']])\n",
    "\n",
    "svm_model = LinearSVC(C=1.0, random_state=0)\n",
    "svm_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Save advanced model\n",
    "dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "dump(svm_model, \"svm_model.pkl\")\n",
    "\n",
    "y_pred_tfidf=svm_model.predict(X_test_tfidf)\n",
    "y_pred_train_tfidf=svm_model.predict(X_train_tfidf)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy_tfidf =accuracy_score(y_pred_tfidf, y_test_tfidf)\n",
    "f1_tfidf =f1_score(y_pred_train_tfidf, y_train_tfidf )\n",
    "precision_tfidf = precision_score(y_test_tfidf, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test_tfidf, y_pred_tfidf)\n",
    "\n",
    "print(\"Model performance after integrating Articles_Clara dataset:\")\n",
    "print(f\"Accuracy: {accuracy_tfidf:.2%}\")\n",
    "print(f\"F1 Score: {f1_tfidf:.2%}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm_tfidf = confusion_matrix(y_test_tfidf, y_pred_tfidf)\n",
    "\n",
    "# Extract TP, TN, FP, FN\n",
    "tn, fp, fn, tp = cm_tfidf.ravel()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_tfidf)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_tfidf, y_pred_tfidf))\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "# Create confusion matrix for SVM model\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_tfidf, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Reliable\", \"Fake\"], yticklabels=[\"Reliable\", \"Fake\"])\n",
    "plt.title(\"Confusion Matrix - SVM model - Fake News\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "### Task 1, Task 2 & Task 3\n",
    "\n",
    "We load our trained models and their vectorizers by using load function in the model `joblib`. We tweaked our earlier approach to fit the LIAR dataset. We made sure that we are able to find the label and its text.\n",
    "\n",
    "We visualised the results of the simple and advanced model by plotting the Confusion Matrix for LIAR. Thus we are able to compare results and make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Load trained models \n",
    "vectorizer = load(\"count_vectorizer.pkl\")\n",
    "clf = load(\"logistic_model.pkl\")\n",
    "vectorizer_tfidf = load(\"tfidf_vectorizer.pkl\")\n",
    "svm_model = load(\"svm_model.pkl\")\n",
    "\n",
    "# Load LIAR dataset\n",
    "liar_train = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/liar/train.tsv\", sep='\\t', on_bad_lines='skip', engine='python', encoding='utf-8')\n",
    "liar_test = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/liar/test.tsv\", sep='\\t', on_bad_lines='skip', engine='python', encoding='utf-8')\n",
    "liar_val = pd.read_csv(\"/Users/clarabovingmagnussen/Desktop/liar/valid.tsv\", sep='\\t', on_bad_lines='skip', engine='python', encoding='utf-8')\n",
    "\n",
    "# Define categorization function \n",
    "def categorize_type(df):\n",
    "    false = {'false', 'barely-true', 'pants-fire'}\n",
    "    true = {'true', 'mostly-true', 'half-true'}\n",
    "\n",
    "    label_col = df.columns[1]\n",
    "    df_categorized = df[df[label_col].isin(false.union(true))].copy()\n",
    "    df_categorized[label_col] = df_categorized[label_col].map(lambda x: 'false' if x in false else 'true')\n",
    "    return df_categorized\n",
    "\n",
    "# Categorize LIAR data\n",
    "categorize_train = categorize_type(liar_train)\n",
    "categorize_test = categorize_type(liar_test)\n",
    "\n",
    "# Extract content and labels\n",
    "text_col = liar_test.columns[2] \n",
    "label_col = liar_test.columns[1] \n",
    "\n",
    "X_test_liar_texts = categorize_test[text_col]\n",
    "y_test_liar = np.array([1 if label == \"false\" else 0 for label in categorize_test[label_col]])\n",
    "\n",
    "# Transform LIAR test data using the FakeNewsCorpus vectorizers\n",
    "X_test_liar_count = vectorizer.transform(X_test_liar_texts)\n",
    "X_test_liar_tfidf = vectorizer_tfidf.transform(X_test_liar_texts)\n",
    "\n",
    "# Predict with pre-trained models \n",
    "y_pred_simple_liar = clf.predict(X_test_liar_count)\n",
    "y_pred_advanced_liar = svm_model.predict(X_test_liar_tfidf)\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    return f1, accuracy, precision, recall, cm, tn, fp, fn, tp\n",
    "\n",
    "# Evaluate simple logistic regression on LIAR dataset\n",
    "f1_simple_L, acc_simple_L, prec_simple_L, rec_simple_L, cm_simple_L, tn_simple_L, tp_simple_L, fp_simple_L, fn_simple_L = evaluate_model(\"Simple Logistic Regression (LIAR)\", y_test_liar, y_pred_simple_liar)\n",
    "\n",
    "# Evaluate advanced SVM on LIAR dataset\n",
    "f1_advanced_L, acc_advanced_L, prec_advanced_L, rec_advanced_L, cm_advanced_L, tn_advanced_L, tp_advanced_L, fp_advanced_L, fn_advanced_L = evaluate_model(\"Advanced SVM (TF-IDF) (LIAR)\", y_test_liar, y_pred_advanced_liar)\n",
    "\n",
    "table = {\n",
    "    'Simple-FN': [f1, accuracy, precision, recall],\n",
    "    'Advanced-FN': [f1_tfidf, accuracy_tfidf, precision_tfidf, recall_tfidf],\n",
    "    'Simple-LIAR': [f1_simple_L, acc_simple_L, prec_simple_L, rec_simple_L],\n",
    "    'Advanced-LIAR': [f1_advanced_L, acc_advanced_L, prec_advanced_L, rec_advanced_L]\n",
    "}\n",
    "\n",
    "# Header column\n",
    "header_column = ['F1 Score', 'Accuracy', 'Precision', 'Recall']\n",
    "\n",
    "# Create the DataFrame with 'header_column' as index\n",
    "table_data = pd.DataFrame(table, index=header_column)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(table_data)\n",
    "\n",
    "# Visualize the confusion matrix for the simple model for LIAR\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_simple_L, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=[\"Reliable\", \"Fake\"], yticklabels=[\"True\", \"False\"])\n",
    "plt.title(\"CM - Simple Model - LIAR\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize the confusion matrix for the advanced model for LIAR\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_simple_L, annot=True, fmt=\"d\", cmap=\"Reds\", xticklabels=[\"Reliable\", \"Fake\"], yticklabels=[\"True\", \"False\"])\n",
    "plt.title(\"CM - Advanced Model - LIAR\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
